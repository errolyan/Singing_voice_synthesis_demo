{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of Neural network-based singing voice synthesis demo using kiritan_singing database (Japanese).ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "VWhj3SHGRShX"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQjZmX2nQLfR"
      },
      "source": [
        "[link text](https://)# 基于神经网络的歌声合成Demo \n",
        "1. 语言：日语\n",
        "2. 数据集：[kiritan_singing database](https://zunko.jp/kiridev/login.php)\n",
        "3. 输入musicxml file,输出音频。\n",
        "4. [nnsvs](https://github.com/r9y9/nnsvs/)\n",
        "5. https://github.com/r9y9/nnsvs/tree/master/egs/kiritan_singing.\n",
        "6. 运行时间5分钟\n",
        "\n",
        "\n",
        "## 提示\n",
        "\n",
        "这个是demo版本，入门歌声合成学习。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oi7WL3T1-H9w"
      },
      "source": [
        "## 下载 music xml 文件"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4UVgGL4-aOv"
      },
      "source": [
        "! git clone -q https://github.com/r9y9/kiritan_singing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWhj3SHGRShX"
      },
      "source": [
        "## 安装要求\n",
        "\n",
        "nnsvs 依赖 sinsy (C++ library) for the muxicxml to context feature conversion. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzyX0v8HRrCS"
      },
      "source": [
        "! pip install -q -U numpy cython\n",
        "! rm -rf hts_engine_API sinsy pysinsy nnmnkwii nnsvs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bh5_HHAdPyUA"
      },
      "source": [
        "# Binary dependencies\n",
        "! git clone -q https://github.com/r9y9/hts_engine_API\n",
        "! cd hts_engine_API/src && ./waf configure --prefix=/usr/ && sudo ./waf build > /dev/null 2>&1 && ./waf install\n",
        "! git clone -q https://github.com/r9y9/sinsy\n",
        "! cd sinsy/src/ && mkdir -p build && cd build && cmake -DCMAKE_BUILD_TYPE=Release -DBUILD_SHARED_LIBS=ON -DCMAKE_INSTALL_PREFIX=/usr/ .. && make -j > /dev/null 2>&1 && sudo make install\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VACHstV2RjAm"
      },
      "source": [
        "# Python dependencies\n",
        "! git clone -q https://github.com/r9y9/pysinsy\n",
        "! cd pysinsy && export SINSY_INSTALL_PREFIX=/usr/ && pip install -q .\n",
        "! git clone -q https://github.com/r9y9/nnmnkwii\n",
        "! cd nnmnkwii && pip install -q .\n",
        "! git clone -q https://github.com/r9y9/nnsvs \n",
        "! cd nnsvs && pip install -q ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3p5KDOHy8OnC"
      },
      "source": [
        "## 导入包"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgpDInEI0tVS"
      },
      "source": [
        "%pylab inline\n",
        "rcParams[\"figure.figsize\"] = (16,5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qX8FC_XlSPlh"
      },
      "source": [
        "import hydra\n",
        "from omegaconf import DictConfig, OmegaConf\n",
        "import numpy as np\n",
        "import joblib\n",
        "import torch\n",
        "from os.path import join, basename, exists\n",
        "import os\n",
        "import pysptk\n",
        "import pyworld\n",
        "import librosa\n",
        "import librosa.display\n",
        "import IPython\n",
        "from IPython.display import Audio\n",
        "from nnmnkwii.io import hts\n",
        "from nnmnkwii import paramgen\n",
        "from nnmnkwii.preprocessing.f0 import interp1d\n",
        "from nnmnkwii.frontend import merlin as fe\n",
        "\n",
        "from nnsvs.multistream import multi_stream_mlpg, split_streams\n",
        "from nnsvs.gen import (\n",
        "    predict_timelag, predict_duration, predict_acoustic, postprocess_duration,\n",
        "    gen_waveform, get_windows)\n",
        "from nnsvs.frontend.ja import xml2lab, _lazy_init\n",
        "from nnsvs.gen import _midi_to_hz\n",
        "\n",
        "_lazy_init(dic_dir=\"/usr/lib/sinsy/dic\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03R6EYp4ULok"
      },
      "source": [
        "sample_rate = 48000\n",
        "frame_period = 5\n",
        "fftlen = pyworld.get_cheaptrick_fft_size(sample_rate)\n",
        "alpha = pysptk.util.mcepalpha(sample_rate)\n",
        "hop_length = int(0.001 * frame_period * sample_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdCvZzUt0xPy"
      },
      "source": [
        "## 建模"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lfMonh6Vi2z"
      },
      "source": [
        "! curl -q -LO https://www.dropbox.com/s/pctlausq00eecqp/20200502_kiritan_singing-00-svs-world.zip\n",
        "! unzip -qq -o 20200502_kiritan_singing-00-svs-world.zip\n",
        "\n",
        "model_dir = \"./20200502_kiritan_singing-00-svs-world\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTnJXxz4VjZu"
      },
      "source": [
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSluQysG06pZ"
      },
      "source": [
        "### Time-lag model 时间模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvS-2DXxZ5SI"
      },
      "source": [
        "timelag_config = OmegaConf.load(join(model_dir, \"timelag\", \"model.yaml\"))\n",
        "timelag_model = hydra.utils.instantiate(timelag_config.netG).to(device)\n",
        "checkpoint = torch.load(join(model_dir, \"timelag\", \"latest.pth\"), map_location=lambda storage, loc: storage)\n",
        "timelag_model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "timelag_in_scaler = joblib.load(join(model_dir, \"in_timelag_scaler.joblib\"))\n",
        "timelag_out_scaler = joblib.load(join(model_dir, \"out_timelag_scaler.joblib\"))\n",
        "timelag_model.eval();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VoPGoAR3kpV"
      },
      "source": [
        "### Duration model  时长模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pl8Q5DgP1eIg"
      },
      "source": [
        "duration_config = OmegaConf.load(join(model_dir, \"duration\", \"model.yaml\"))\n",
        "duration_model = hydra.utils.instantiate(duration_config.netG).to(device)\n",
        "checkpoint = torch.load(join(model_dir, \"duration\", \"latest.pth\"), map_location=lambda storage, loc: storage)\n",
        "duration_model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "duration_in_scaler = joblib.load(join(model_dir, \"in_duration_scaler.joblib\"))\n",
        "duration_out_scaler = joblib.load(join(model_dir, \"out_duration_scaler.joblib\"))\n",
        "duration_model.eval();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2flghYb3q-o"
      },
      "source": [
        "### Acoustic model 语音建模"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwmBKAE83pYG"
      },
      "source": [
        "acoustic_config = OmegaConf.load(join(model_dir, \"acoustic\", \"model.yaml\"))\n",
        "acoustic_model = hydra.utils.instantiate(acoustic_config.netG).to(device)\n",
        "checkpoint = torch.load(join(model_dir, \"acoustic\", \"latest.pth\"), map_location=lambda storage, loc: storage)\n",
        "acoustic_model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "acoustic_in_scaler = joblib.load(join(model_dir, \"in_acoustic_scaler.joblib\"))\n",
        "acoustic_out_scaler = joblib.load(join(model_dir, \"out_acoustic_scaler.joblib\"))\n",
        "acoustic_model.eval();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omkQ2G4i4DZN"
      },
      "source": [
        "## 合成"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7b9Yc05CQSu"
      },
      "source": [
        "### 选取xml文件"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_V1yQdEe4MTq"
      },
      "source": [
        "# NOTE: 01.xml and 02.xml were not included in the training data\n",
        "# 03.xml - 37.xml were used for training.\n",
        "labels = xml2lab(\"kiritan_singing/musicxml/01.xml\").round_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-HvucB14TFS"
      },
      "source": [
        "question_path = join(model_dir, \"jp_qst001_nnsvs.hed\")\n",
        "binary_dict, continuous_dict = hts.load_question_set(question_path, append_hat_for_LL=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8yAgiw36fCZ"
      },
      "source": [
        "# pitch indices in the input features\n",
        "pitch_idx = len(binary_dict) + 1\n",
        "pitch_indices = np.arange(len(binary_dict), len(binary_dict)+3)\n",
        "log_f0_conditioning = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4q5INSdh6lVj"
      },
      "source": [
        "### Predict time-lag"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXzv451B6jS6"
      },
      "source": [
        "lag = predict_timelag(device, labels, timelag_model, timelag_in_scaler,\n",
        "    timelag_out_scaler, binary_dict, continuous_dict, pitch_indices,\n",
        "    log_f0_conditioning)\n",
        "lag.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQr9u_uv6lzg"
      },
      "source": [
        "plot(lag / 50000, label=\"Timelag (in frames) for note onsets\", linewidth=2)\n",
        "xlabel(\"Time index in musical note\")\n",
        "ylabel(\"Timelag\")\n",
        "legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OALA70D6sHy"
      },
      "source": [
        "### Predict phoneme durations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gffqkEYU6o5F"
      },
      "source": [
        "durations = predict_duration(device, labels, duration_model,\n",
        "    duration_in_scaler, duration_out_scaler, lag, binary_dict, continuous_dict,\n",
        "    pitch_indices, log_f0_conditioning)\n",
        "durations.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnoRw3Ah6tX1"
      },
      "source": [
        "plot(durations, label=\"Phoneme durations in frames\")\n",
        "xlabel(\"Time index in phone\")\n",
        "ylabel(\"Duration\")\n",
        "legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIBV5ZJj6xPv"
      },
      "source": [
        "# Normalize phoneme durations to satisfy constraints by the musical score\n",
        "duration_modified_labels = postprocess_duration(labels, durations, lag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0_rgqQg6v77"
      },
      "source": [
        "### Predict acoustic features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAuA45p26uP_"
      },
      "source": [
        "acoustic_features = predict_acoustic(device, duration_modified_labels, acoustic_model,\n",
        "    acoustic_in_scaler, acoustic_out_scaler, binary_dict, continuous_dict,\n",
        "    \"coarse_coding\", pitch_indices, log_f0_conditioning)\n",
        "acoustic_features.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0dkvEJK6318"
      },
      "source": [
        "### Visualize acoustic features\n",
        "\n",
        "Before generating a wavefrom, let's visualize acoustic features to understand how the acoustic model works. Since acoustic features contain multiple differnt features (*multi-stream*, e.g., mgc, lf0, vuv and bap), let us first split acoustic features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coYE3tqF61tQ"
      },
      "source": [
        "stream_sizes = acoustic_config.stream_sizes\n",
        "has_dynamic_features = acoustic_config.has_dynamic_features\n",
        "# (mgc, lf0, vuv, bap) with delta and delta-delta except for vuv\n",
        "stream_sizes, has_dynamic_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuCrC7GY65Ls"
      },
      "source": [
        "feats = multi_stream_mlpg(\n",
        "    acoustic_features, acoustic_out_scaler.var_, get_windows(3), stream_sizes,\n",
        "    has_dynamic_features)\n",
        "# get static features\n",
        "mgc, diff_lf0, vuv, bap = split_streams(feats, [60, 1, 1, 5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ma-6HPn068Kb"
      },
      "source": [
        "#### Visualize F0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0OPYJoT6511"
      },
      "source": [
        "# relative f0 -> absolute f0\n",
        "# need to extract pitch sequence from the musical score\n",
        "linguistic_features = fe.linguistic_features(duration_modified_labels,\n",
        "                                            binary_dict, continuous_dict,\n",
        "                                            add_frame_features=True,\n",
        "                                            subphone_features=\"coarse_coding\")\n",
        "f0_score = _midi_to_hz(linguistic_features, pitch_idx, False)[:, None]\n",
        "lf0_score = f0_score.copy()\n",
        "nonzero_indices = np.nonzero(lf0_score)\n",
        "lf0_score[nonzero_indices] = np.log(f0_score[nonzero_indices])\n",
        "lf0_score = interp1d(lf0_score, kind=\"slinear\")\n",
        "\n",
        "f0 = diff_lf0 + lf0_score\n",
        "f0[vuv < 0.5] = 0\n",
        "f0[np.nonzero(f0)] = np.exp(f0[np.nonzero(f0)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwqgIQfp69QJ"
      },
      "source": [
        "plot(f0[-2500:, :], linewidth=2, label=\"F0 contour (in Hz)\")\n",
        "plot((vuv[-2500:, :] > 0.5)*100, linewidth=2, label=\"Voiced/unvoiced flag\")\n",
        "legend()\n",
        "xlabel(\"Frame\")\n",
        "ylabel(\"F0 (in Hz)\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smlsejRv7ByF"
      },
      "source": [
        "#### Visualize spectrogram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdsyUi3u6-Rt"
      },
      "source": [
        "# Trim and visualize (to save memory and time)\n",
        "logsp = np.log(pysptk.mc2sp(mgc[-2500:, :], alpha=alpha, fftlen=fftlen))\n",
        "librosa.display.specshow(logsp.T, sr=sample_rate, hop_length=hop_length, x_axis=\"time\", y_axis=\"linear\", cmap=\"jet\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpnhK_Um7FPX"
      },
      "source": [
        "#### Visualize aperiodicity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUCWh5Er7DAU"
      },
      "source": [
        "aperiodicity = pyworld.decode_aperiodicity(bap[-2500:, :].astype(np.float64), sample_rate, fftlen)\n",
        "librosa.display.specshow(aperiodicity.T, sr=sample_rate, hop_length=hop_length, x_axis=\"time\", y_axis=\"linear\", cmap=\"jet\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_i8cFqle7Ibm"
      },
      "source": [
        "### Generate waveform\n",
        "\n",
        "Finally, let's generate waveform and listen to the sample."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLMUQa2c7Gei"
      },
      "source": [
        "generated_waveform = gen_waveform(\n",
        "    duration_modified_labels, acoustic_features, acoustic_out_scaler,\n",
        "    binary_dict, continuous_dict, acoustic_config.stream_sizes,\n",
        "    acoustic_config.has_dynamic_features,\n",
        "    \"coarse_coding\", log_f0_conditioning,\n",
        "    pitch_idx, num_windows=3,\n",
        "    post_filter=True, sample_rate=sample_rate, frame_period=frame_period,\n",
        "    relative_f0=True)\n",
        "\n",
        "# trim trailing/leading silences for covenience\n",
        "generated_waveform = librosa.effects.trim(generated_waveform)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYSyTQylDvIR"
      },
      "source": [
        "## Listen to the generated sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-qNFhnu7J1T"
      },
      "source": [
        "librosa.display.waveplot(generated_waveform, sample_rate, x_axis=\"time\")\n",
        "IPython.display.display(Audio(generated_waveform, rate=sample_rate))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a2Q8N487MPZ"
      },
      "source": [
        "## Summary\n",
        "\n",
        "A demo of a singing voice synthesis system based on neural networks. Full code is available https://github.com/r9y9/nnsvs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XQ_diETC6FB"
      },
      "source": [
        "\n",
        "## References\n",
        "\n",
        "- Kiritan database: https://zunko.jp/kiridev/login.php\n",
        "- Code to reproduce: https://github.com/r9y9/nnsvs"
      ]
    }
  ]
}